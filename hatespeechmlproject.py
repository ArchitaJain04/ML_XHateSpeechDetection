# -*- coding: utf-8 -*-
"""HateSpeechMLProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K8MMsrYmbqwzyWghq3Up8331Qf2Vh2rQ
"""

import pandas as pd
# Provides functionalities for handling and manipulating data in DataFrames

from sklearn.utils import resample
# Used for upsampling or downsampling data, often to handle imbalanced datasets
# Upsampling : Increasing the number of instances in the minority class by duplicating existing samples
# Downsampling : Reducing the number of instances in the majority class by randomly removing samples

from wordcloud import WordCloud
# Displaying how often different words appear in a given text dataset using visual tools like a word cloud

import matplotlib.pyplot as plt
# For creating static, interactive, and animated plots

import seaborn as sns
# Enhances matplotlib with easier plotting and built-in themes

import numpy as np
# Provides support for arrays, mathematical operations, and linear algebra

import re
# For text processing and pattern matching. Commonly used for Data cleaning (removing unwanted characters, URLs.)

from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer
# TfidfVectorizer : Converts text data into a matrix of TF-IDF features (TF : Term Frequency , IDF : Inverse Document Frequency)
# TfidfTransformer : Converts text data into a matrix of TF-IDF values
# CountVectorizer : Converts text into a matrix of word frequencies.

from sklearn.model_selection import train_test_split
# Splits data into random training and testing subsets.

from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score
from sklearn import metrics
# Performance evaluation metrics for ML models.

from sklearn.compose import ColumnTransformer
# To apply transformations to specific columns in a dataset

from sklearn.preprocessing import OneHotEncoder
# Data preprocessing and scaling
# OneHotEncoder : Encodes categorical features as binary vectors.

from sklearn.linear_model import LogisticRegression
# Implements logistic regression for binary or multi-class classification

from sklearn import svm
# Support Vector Machine models for classification and regression.
# Logistic Regression : statistical method used for binary classification problems â€”  yes/no, 0/1, or spam/not spam

import time
# Used for measuring execution time or handling delays.


# STEP 1 : LOADING THE DATASET (For ease we took the dataset in 2 parts one for testing and one for training)
train = pd.read_csv('/content/train_E6oV3lV.csv')
print("Training Set 1 :"% train.columns, train.shape)
test = pd.read_csv('/content/test_tweets_anuFYb8.csv')
print("Test Set 1 :"% test.columns, test.shape)

#STEP 2 : CHECKING FOR NULL VALUES AND CLEANING THE DATA IN THE DATASET
# Handling null values
print('Train Set  -----')
print(train.isnull().sum())
print('Test Set  -----')
print(test.isnull().sum())
train.head()

# df : A pandas DataFrame
# text_field : The name of the column in the DataFrame containing text to clean
def  clean_text(df, text_field):
    df[text_field] = df[text_field].str.lower() #lowercase every word. Removing case sensitivity
    df[text_field] = df[text_field].apply(lambda elem: re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", elem))
    # Removes unnecessary symbols, mentions, URLs, and retweets.
    return df
test_clean = clean_text(test, "tweet")
train_clean = clean_text(train, "tweet")

# Filters the train_clean DataFrame to select rows where the label == 0 (majority) and label == 1 (minority)
train_majority = train_clean[train_clean.label==0]
train_minority = train_clean[train_clean.label==1]
train_minority_upsampled = resample(train_minority, replace=True, n_samples=len(train_majority), random_state=123)
train_upsampled = pd.concat([train_minority_upsampled, train_majority])
# Combines the upsampled minority class and the original majority class into a new DataFrame 'train_upsampled'
train_upsampled['label'].value_counts() # Counts the number of occurrences of each label (0 and 1) in the train_upsampled
train_upsampled


# STEP 3 : Visually representing the most common words in hate comments and non-hate comments
fig, axs = plt.subplots(1,2 , figsize=(16,8)) #Specifies a grid with 1 row and 2 columns and size 16x8 inches

text_pos = " ".join(train_upsampled['tweet'][train.label == 0]) # select rows where the label is 0
text_neg = " ".join(train_upsampled['tweet'][train.label == 1]) # select rows where the label is 1

train_cloud_pos = WordCloud(collocations = False, background_color = 'white').generate(text_pos)
# Generates a visual representation of the most common words in non-hate comments.
train_cloud_neg = WordCloud(collocations = False, background_color = 'black').generate(text_neg)
# Generates a visual representation of the most common words in hate comments.
axs[0].imshow(train_cloud_pos, interpolation='bilinear') # Displays the non-hate comments word cloud on the first subplot (axs[0])
axs[0].axis('off')
axs[0].set_title('Non-Hate Comments')
axs[1].imshow(train_cloud_neg, interpolation='bilinear') # Displays the hate comments word cloud on the first subplot (axs[0])
axs[1].axis('off')
axs[1].set_title('Hate Comments')
plt.show()


# STEP 4 : FEATURE SELECTION AND DATA TRANSFORMATION
dt_trasformed = train_upsampled[['label', 'tweet']] # Selects columns label and tweet
y = dt_trasformed.iloc[:, :-1].values
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')
# Applies OneHotEncoder to the first column ([0]), which is the label column.
# OneHotEncoding : Involves converting categorical variables into a series of binary (0 or 1) columns.
y = np.array(ct.fit_transform(y))
print("Dimensions of the transformed array are : " , y.shape)
y_df = pd.DataFrame(y)
y_hate = np.array(y_df[0])
y

# To avoid overfitting
cv = CountVectorizer(max_features = 2000)
# Converts the text data into a bag-of-words model, which counts the frequency of each word in the dataset (2000 most frequently used words)
x = cv.fit_transform(train_upsampled['tweet']).toarray()
print("Dimensions of the vectorized array are : " , x.shape)


#STEP 5 : CREATING TEST AND TRAIN SETS FOR TRAINING AND EVALUATION
x_train, x_test, y_train, y_test = train_test_split(x, y_hate, test_size = 0.30, random_state = 42)

# Initialize the model and note start time
classifier_lr = LogisticRegression(random_state = 0)
start_time = time.time()

# Train the model and note end time
classifier_lr.fit(x_train, y_train)
end_time = time.time()

#Calculate the training time
training_time = end_time - start_time
print("Training time of Logistic Regression: " , training_time , "seconds")


# STEP 6 : EVALUATION MEASURES
y_pred_lr=classifier_lr.predict(x_test)
cm = confusion_matrix(y_test, y_pred_lr) # Confusion Matrix
print("Confusion Matrix : ")
print(cm)

cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, y_pred_lr, labels = [1,0]), display_labels = [True, False])
cm_display.plot()
plt.show()

# Accuracy
accuracy = accuracy_score(y_test, y_pred_lr)
print("Accuracy : " , accuracy)

# Precision
precision = precision_score(y_test, y_pred_lr)
print("Precision : " , precision)

# Recall
recall = recall_score(y_test, y_pred_lr)
print("Recall : " , recall)

# F1-Score
f1 = f1_score(y_test, y_pred_lr)
print("F1-Score : " , f1)